[
  {
    "id": "PERF-001",
    "title": "Avoid repeated color piece ORs via cached occupancy bitboards",
    "priority": "high",
    "category": "memory",
    "description": "",
    "status": "failed",
    "created_at": "2026-02-14T17:59:42.869597",
    "completed_at": "2026-02-14T18:26:07.497242",
    "estimated_complexity": "medium",
    "files_affected": [
      "bitboard/src/position.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "Functions like Position::our_pieces, their_pieces and or_color recompute color occupancy by OR-ing six piece bitboards every time, even though OccupancyMap already maintains color and both-side occupancies.",
    "proposed_optimization": "Use OccupancyMap directly for color occupancies (OccupancyKind::White/Black) and maintain separate non-pawn or piece-type aggregates only when needed, avoiding repeated OR chains on hot paths such as movegen and attack checks.",
    "expected_speedup": "10-20%",
    "requires_unsafe": "no",
    "requires_benchmarking": "yes",
    "reasoning": "our_pieces/their_pieces are used in MoveGenContext and attack checks and are currently computed via six ORs and piece lookups. This redundant work is avoidable because OccupancyMap already keeps combined color sets up to date in apply_move_into/set_piece. Replacing these recomputations with simple reads from OccupancyMap will reduce arithmetic and memory traffic in tight loops, improving cache usage and lowering instruction count.",
    "measurement_approach": "Add microbenchmarks around generate_legal_moves and perft at depths 4\u20136 on several FENs, comparing nodes per second and total cycles (using perf or VTune) before/after. Also profile NODE_COUNT/time in engine::bench::bench_search."
  },
  {
    "id": "PERF-002",
    "title": "Inline and simplify BitBoardMask helpers to reduce overhead",
    "priority": "medium",
    "category": "rust_specific",
    "description": "",
    "status": "in-progress",
    "created_at": "2026-02-14T17:59:42.869597",
    "completed_at": null,
    "estimated_complexity": "small",
    "files_affected": [
      "bitboard/src/bitboardmask.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "BitBoardMask methods like contains_square, and, or and from_square add extra call/abstraction layers and sometimes reimplement simple bit operations, which can inhibit inlining and generate redundant masking in hot paths such as movegen and legality checks.",
    "proposed_optimization": "Mark the hottest BitBoardMask methods #[inline(always)] (contains_square, is_nonempty, from_square, squares, first_square) and replace calls to ad-hoc bit patterns with direct u64 operations where possible. Remove duplicate methods (count vs count_ones) and prefer inherent methods on u64 to help LLVM optimize.",
    "expected_speedup": "5-10%",
    "requires_unsafe": "no",
    "requires_benchmarking": "yes",
    "reasoning": "Movegen and attack resolution rely heavily on bitboard operations; small overheads in these helpers accumulate. Ensuring aggressive inlining and eliminating redundant masking encourages the optimizer to fuse operations and keeps data in registers. Reducing API surface avoids unnecessary layers that the optimizer must see through.",
    "measurement_approach": "Use cargo bench to compare knight/rook/bishop movegen benches and perft throughput. Collect -Zself-profile or perf data to confirm reduced instruction counts in BitBoardMask-heavy functions like generate_pseudo_* and quiescence_with_arena."
  },
  {
    "id": "PERF-003",
    "title": "Remove O(64) Square::all_array scans in generate_pseudo_captures",
    "priority": "high",
    "category": "move_gen",
    "description": "",
    "status": "in-progress",
    "created_at": "2026-02-14T17:59:42.869597",
    "completed_at": null,
    "estimated_complexity": "medium",
    "files_affected": [
      "bitboard/src/movegen/captures.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "Pawn capture generation loops over Square::all_array() and uses pawn_attacks_to(to, us) per target square, resulting in an O(64) scan per call instead of iterating only over existing pawns.",
    "proposed_optimization": "Rewrite pawn capture generation to iterate over pawn bitboard and shift it by attack directions (like in pawn.rs) intersected with their_occ, using bit scans rather than a full board loop.",
    "expected_speedup": "2x",
    "requires_unsafe": "no",
    "requires_benchmarking": "yes",
    "reasoning": "Captures are used in quiescence and selective search. The current design computes attackers for each of 64 squares, many of which are empty, and repeatedly indexes PAWN_ATTACKS. Using directional shifts from the pawn set, as already done in normal pawn movegen, restricts work to actual pawns and uses simple bit ops. This can drastically reduce operations in capture-only search and improve branch predictability.",
    "measurement_approach": "Add a microbenchmark for generate_pseudo_captures on tactical FENs and compare time and allocations. Reprofile quiescence_with_arena to verify the capture generation portion shrinks in time and samples."
  },
  {
    "id": "PERF-004",
    "title": "Avoid repeated is_square_attacked calls in castling generation",
    "priority": "medium",
    "category": "move_gen",
    "description": "",
    "status": "failed",
    "created_at": "2026-02-14T17:59:42.869597",
    "completed_at": "2026-02-15T03:36:08.324669",
    "estimated_complexity": "medium",
    "files_affected": [
      "bitboard/src/position.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "can_castle_kingside/queenside call is_square_attacked multiple times (for king and transit squares), which recomputes attackers from scratch for each square, increasing overhead for castling checks in movegen and legality.",
    "proposed_optimization": "Factor a helper that computes all enemy attacks on king-adjacent squares in one pass using attack tables and occupancy, or reuse a cached attack mask from legality code to test all three squares with bit tests instead of separate function calls.",
    "expected_speedup": "10-20%",
    "requires_unsafe": "no",
    "requires_benchmarking": "yes",
    "reasoning": "Castling availability is checked every time king moves are generated. Each is_square_attacked call recomputes potential attackers (pawns, knights, sliders) leading to ~3x redundant work. Computing an aggregate attacked mask once and testing multiple squares with bitwise ANDs reduces repeated work and avoids control-flow overhead inside is_square_attacked.",
    "measurement_approach": "Profile generate_legal_moves on positions with many castling rights (test_data and perft) before/after. Measure CPU cycles spent in is_square_attacked and can_castle_* via perf or flamegraph, confirming reduction."
  },
  {
    "id": "PERF-005",
    "title": "Make occupancy_to_index use portable fallback and mark inline(always)",
    "priority": "medium",
    "category": "move_gen",
    "description": "",
    "status": "failed",
    "created_at": "2026-02-14T17:59:42.869597",
    "completed_at": "2026-02-15T03:48:38.988397",
    "estimated_complexity": "medium",
    "files_affected": [
      "bitboard/src/bitboard.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "occupancy_to_index is only compiled on x86_64 with BMI2 and uses unsafe _pext_u64 directly. On targets without BMI2 this function is absent, leading to potential slower alternatives inside callers or compilation issues. Also, indexing functions call it through an extra layer.",
    "proposed_optimization": "Provide a portable fallback implementation using software PEXT (bitscan & masking) when BMI2 is unavailable, and mark occupancy_to_index and callers rook_attacks_from/bishop_attacks_from #[inline(always)]. Optionally gate runtime detection with is_x86_feature_detected and store a function pointer.",
    "expected_speedup": "minor",
    "requires_unsafe": "yes",
    "requires_benchmarking": "yes",
    "reasoning": "Ensuring a single, aggressively inlined occupancy_to_index for all sliding attacks reduces call overhead and allows better constant propagation in attack lookups. A good software PEXT can be quite fast on non-BMI2 CPUs. Removing cfg gaps avoids fallback to slower, non-magic sliding logic if any exists elsewhere.",
    "measurement_approach": "Add a microbenchmark for rook_attacks_from/bishop_attacks_from across random occupancies, compare cycles on machines with and without BMI2. Use perf to inspect instruction counts and branch behavior around the occupancy_to_index hot path."
  },
  {
    "id": "PERF-006",
    "title": "Simplify subray diagonal functions using precomputed masks",
    "priority": "low",
    "category": "algorithmic",
    "description": "",
    "status": "not-started",
    "created_at": "2026-02-14T17:59:42.869597",
    "completed_at": null,
    "estimated_complexity": "large",
    "files_affected": [
      "bitboard/src/bitboardmask.rs",
      "bitboard/src/bitboard.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "Diagonal subray functions (subray_up_right, subray_down_left, etc.) walk rays stepwise with loops and in-loop BitBoardMask constructions, which are slower than table-based magic or mask approaches when used in bishop_attacks_from.",
    "proposed_optimization": "Leverage existing DIAGONAL_MASKS/ANTIDIAGONAL_MASKS and magic tables exclusively for sliding attacks, and either remove or restrict use of the iterative subray_* functions from performance-critical contexts; if needed, reimplement them using hyperbola quintessence style arithmetic on per-diagonal compressed indices.",
    "expected_speedup": "minor",
    "requires_unsafe": "no",
    "requires_benchmarking": "yes",
    "reasoning": "The engine already ships large BISHOP_ATTACKS/ROOK_ATTACKS tables and bishop_attacks_from that use PEXT; diagonal subray routines appear redundant or slower. Ensuring high-level attack generation always uses the fastest precomputed approach avoids extra branching and bit-iteration, improving movegen throughput, though gains might be modest since main sliders already use tables.",
    "measurement_approach": "Search the codebase for uses of subray_* in non-const contexts and replace them with bishop_attacks_from/rook_attacks_from. Benchmark perft and slider-only generation to confirm no regressions; use perf to confirm diagonal loops disappear from hot profiles."
  },
  {
    "id": "PERF-007",
    "title": "Speed up Position::from_fen with fixed-size arrays and no Vec",
    "priority": "low",
    "category": "rust_specific",
    "description": "",
    "status": "not-started",
    "created_at": "2026-02-14T17:59:42.869597",
    "completed_at": null,
    "estimated_complexity": "medium",
    "files_affected": [
      "bitboard/src/position.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "Position::from_fen allocates a Vec<String> via split_whitespace().collect(), performs multiple string operations, and panics on invalid input, increasing overhead when FEN parsing is used frequently in tests and benchmarks.",
    "proposed_optimization": "Parse FEN into fixed six slices using str::splitn and avoid Vec allocations; operate directly on &str segments and indices for halfmove/fullmove parsing. Optionally use unchecked indexing for well-formed inputs in release builds.",
    "expected_speedup": "minor",
    "requires_unsafe": "no",
    "requires_benchmarking": "no",
    "reasoning": "While FEN parsing is not on the deepest inner loop, test harnesses and UCI integration construct many Positions from FEN (e.g., perft tests, TEST_CASES, uci 'position' command). Removing Vec allocations and string copies reduces allocator pressure and improves startup and test times.",
    "measurement_approach": "Benchmark bulk creation of Positions from test_data::TEST_CASES FENs and mass perft initialization before/after. Track allocations with valgrind massif or Rust\u2019s allocation instrumentation."
  },
  {
    "id": "PERF-008",
    "title": "Avoid Vec allocation for children in Node (Arena-based search)",
    "priority": "medium",
    "category": "memory",
    "description": "",
    "status": "failed",
    "created_at": "2026-02-14T17:59:42.869597",
    "completed_at": "2026-02-15T03:59:55.920058",
    "estimated_complexity": "small",
    "files_affected": [
      "engine/src/core/node.rs",
      "engine/src/core/arena.rs",
      "engine/src/search/search.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "Node contains a Vec<usize> children field even though the search currently does not use it. This adds heap allocations and pointer chasing potential if expanded later, and increases Node size affecting Arena cache locality.",
    "proposed_optimization": "Remove or gate the children Vec behind a feature flag; keep Node as a compact struct with only Position and score for the current engine. If future algorithms need child references, store them in separate side-structures indexed by ply, not per-node Vecs.",
    "expected_speedup": "minor",
    "requires_unsafe": "no",
    "requires_benchmarking": "yes",
    "reasoning": "Arena-based Node storage is designed for cache efficiency. A per-node Vec, even if unused, increases memory footprint and potential allocator calls. Keeping Node lean improves cache behavior when traversing the arena in deep searches and allows more nodes to fit in L1/L2.",
    "measurement_approach": "Measure memory usage and cache-miss metrics using perf stat (LLC-load-misses) during bench_search at depth 5\u20137, comparing with and without the Vec. Verify absence of allocations related to Node children in profiling traces."
  },
  {
    "id": "PERF-009",
    "title": "Use TT best_move for root move ordering more aggressively",
    "priority": "high",
    "category": "search",
    "description": "",
    "status": "failed",
    "created_at": "2026-02-14T17:59:42.869597",
    "completed_at": "2026-02-15T02:46:22.490155",
    "estimated_complexity": "medium",
    "files_affected": [
      "engine/src/search/search.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "probe_for_best_move uses TT only at the root and only once per depth, but TT entries store null best_move at interior nodes and do not propagate PV moves, limiting how much move ordering benefits alpha-beta pruning.",
    "proposed_optimization": "Ensure TT stores the actual best_move at interior nodes where available and prefer it in child move ordering as well as at root. At root, move TT best move to front and leave the rest unsorted; later consider lightweight history heuristics.",
    "expected_speedup": "20-40%",
    "requires_unsafe": "no",
    "requires_benchmarking": "yes",
    "reasoning": "Alpha-beta\u2019s effectiveness is highly dependent on move ordering. Currently, TT stores ChessMove but search_node_with_arena writes ChessMove::null on final store, meaning TT best_move is rarely usable. Storing the actual first-improving move and using it consistently to sort successors will reduce the search tree, especially at deeper iterations, yielding substantial node savings.",
    "measurement_approach": "Instrument NODE_COUNT and elapsed time per depth in bench_search at fixed depth (e.g., 6) before/after. Compare node counts and timing; collect per-depth info from print_uci_info logs to quantify reductions in explored nodes."
  },
  {
    "id": "PERF-010",
    "title": "Reduce quiescence explosion via delta pruning and SEE-like filter",
    "priority": "critical",
    "category": "search",
    "description": "",
    "status": "in-progress",
    "created_at": "2026-02-14T17:59:42.869597",
    "completed_at": null,
    "estimated_complexity": "large",
    "files_affected": [
      "engine/src/search/quiescence.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "quiescence_with_arena expands all captures (and evasions when in check) without any static-exchange or delta-based pruning; combined with MVV/LVA ordering only, this leads to very wide trees on tactical positions.",
    "proposed_optimization": "Introduce basic delta pruning: skip captures where stand_pat + victim_value + margin <= alpha, and optionally a fast static-exchange evaluation (SEE) on captures to discard obviously losing exchanges. Guard these with conservative thresholds to preserve correctness.",
    "expected_speedup": "2-3x",
    "requires_unsafe": "no",
    "requires_benchmarking": "yes",
    "reasoning": "Quiescence search dominates node counts in many engines. Currently, any capture is searched regardless of tactical merit. Delta pruning based on material gain bounds is a standard technique to cut low-potential captures while keeping tactical soundness for most positions. Even a cheap SEE or victim-attacker ordering heuristic can dramatically reduce the branching factor, improving nps and convergence of evals.",
    "measurement_approach": "Track NODE_COUNT spent in quiescence by instrumenting quiescence_with_arena calls and compare total nodes and timing on tactical test cases (e.g., Knight_Maze, queen_infiltration) before/after. Verify evaluation correctness via existing test suite and perft-based consistency checks."
  },
  {
    "id": "PERF-011",
    "title": "Use separate TT per thread to avoid contention and false sharing",
    "priority": "high",
    "category": "memory",
    "description": "",
    "status": "failed",
    "created_at": "2026-02-14T17:59:42.869597",
    "completed_at": "2026-02-15T02:56:21.834107",
    "estimated_complexity": "large",
    "files_affected": [
      "engine/src/search/search.rs",
      "engine/src/core/tt.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "Root-parallel search uses per-thread local TranspositionTable only for that depth, but Engine also maintains a shared optional tt that is unused in parallel branch evaluation. Future concurrent access could suffer from contention and false sharing on TTEntry slots.",
    "proposed_optimization": "Formalize the design as strictly per-thread TT slices: allocate TT per worker (or shard TT into cacheline-aligned buckets per thread) and avoid sharing TTEntry writes among threads; keep a smaller, possibly shared, read-only TT if desired.",
    "expected_speedup": "10-30%",
    "requires_unsafe": "no",
    "requires_benchmarking": "yes",
    "reasoning": "Concurrent random access to a large contiguous TT by multiple threads causes cache line ping-pong and hidden contention, especially if probing and storing involve writes. Although current code uses local TT in the parallel branch, clarifying this pattern and ensuring any shared TT is read-mostly (or partitioned) will improve scalability when num_threads >1, maintaining linear-ish speedups.",
    "measurement_approach": "Run bench_search with varying Threads options (1,2,4,8) and measure scaling of nodes/sec. Use perf to inspect cache misses and LLC-store-misses; compare versions with a global TT vs per-thread TT instances."
  },
  {
    "id": "PERF-012",
    "title": "Avoid frequent file I/O and logging in hot search paths",
    "priority": "medium",
    "category": "search",
    "description": "",
    "status": "failed",
    "created_at": "2026-02-14T17:59:42.869597",
    "completed_at": "2026-02-15T04:10:29.866795",
    "estimated_complexity": "medium",
    "files_affected": [
      "engine/src/search/core.rs",
      "engine/src/search/quiescence.rs",
      "engine/src/api/uciapi.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "print_uci_info and quiescence_with_arena emit debug output and open cody_uci.log via OpenOptions in hot paths when VERBOSE is enabled, causing syscalls, file locks and flushing, which can dwarf computation time.",
    "proposed_optimization": "Gate all debug logging behind a cheap runtime flag check with early return; keep file handles open across calls instead of re-opening per line; buffer logs and flush at coarse intervals rather than per-node or per-depth.",
    "expected_speedup": "minor to 5x (when verbose)",
    "requires_unsafe": "no",
    "requires_benchmarking": "no",
    "reasoning": "I/O is orders of magnitude slower than in-memory search. When VERBOSE is used (e.g., debugging difficult positions), repeated OpenOptions::open and write! calls inside quiescence and print_uci_info will devastate performance, masking search characteristics. Ensuring that I/O is rare and amortized preserves debuggability without sacrificing speed.",
    "measurement_approach": "Run bench_search with VERBOSE on and off; compare total time and system time (via time or perf). Ensure that with optimized logging, enabling VERBOSE has minimal overhead in non-debug-critical runs."
  },
  {
    "id": "PERF-013",
    "title": "Use LRU-like replacement and aging in TT store policy",
    "priority": "low",
    "category": "search",
    "description": "",
    "status": "not-started",
    "created_at": "2026-02-14T17:59:42.870608",
    "completed_at": null,
    "estimated_complexity": "large",
    "files_affected": [
      "engine/src/core/tt.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "TT.store rejects overwriting entries when existing depth is greater than new depth, but otherwise always replaces; there is no aging or generation mechanism, which can waste TT capacity on old, deep entries irrelevant to current root.",
    "proposed_optimization": "Add a generation counter or age value and prefer replacing old entries even if slightly deeper, or use a cluster of entries per index (2\u20134-way associativity) with a replacement heuristic based on depth and age. Make TT size tunable via config.",
    "expected_speedup": "minor",
    "requires_unsafe": "no",
    "requires_benchmarking": "yes",
    "reasoning": "Better TT replacement does not speed up individual probes but increases hit rate for relevant nodes across iterative deepening, indirectly reducing search work. However, benefits depend on position set and depth; improvements are incremental compared to move ordering or quiescence pruning.",
    "measurement_approach": "Instrument TT hit/miss statistics and node counts at fixed depths for representative FENs. Compare hit rates and total nodes before/after improving replacement, especially at higher depths (8\u201310)."
  },
  {
    "id": "PERF-014",
    "title": "Leverage const fn more widely for precomputed tables and masks",
    "priority": "low",
    "category": "rust_specific",
    "description": "",
    "status": "not-started",
    "created_at": "2026-02-14T17:59:42.870608",
    "completed_at": null,
    "estimated_complexity": "medium",
    "files_affected": [
      "bitboard/src/bitboard.rs",
      "bitboard/src/bitboardmask.rs",
      "bitboard/src/square.rs",
      "bitboard/src/tables/*.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "Some helper functions and tables are const but others, like Square::from_rank_file, some bitboard mask builders, and parts of Zobrist initialization use runtime constructs, leading to potential recomputation or missed compile-time folding.",
    "proposed_optimization": "Audit helpers for eligibility as const fn (e.g., small indexing, bit shifts) and refactor to move as much table generation as possible into const contexts, ensuring compile-time evaluation of masks and rays.",
    "expected_speedup": "minor",
    "requires_unsafe": "no",
    "requires_benchmarking": "no",
    "reasoning": "Compile-time evaluation does not affect steady-state search performance directly but reduces startup overhead and guarantees tables are materialized before first use. It also helps the optimizer by providing more constant values at call sites, facilitating inlining and dead-code elimination.",
    "measurement_approach": "Verify via cargo asm or MIR dumps that initialization code is reduced. Optionally benchmark repeated library initialization (new Engine, new Position::default) in tight loops to confirm small startup gains."
  },
  {
    "id": "PERF-015",
    "title": "Enable LTO and tune codegen-units for search crate",
    "priority": "high",
    "category": "compilation",
    "description": "",
    "status": "in-progress",
    "created_at": "2026-02-14T17:59:42.870608",
    "completed_at": null,
    "estimated_complexity": "small",
    "files_affected": [
      "Cargo.toml"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "The build config shown does not explicitly enable Link-Time Optimization (LTO) or adjust codegen-units; default settings can fragment optimization across multiple units, reducing cross-module inlining for hot paths across bitboard and engine crates.",
    "proposed_optimization": "Turn on thin LTO for release builds and lower codegen-units (e.g., 1\u20134) for the engine and bitboard crates. Optionally enable opt-level=\"z\" for non-critical crates but keep opt-level=3 for engine and bitboard.",
    "expected_speedup": "10-25%",
    "requires_unsafe": "no",
    "requires_benchmarking": "yes",
    "reasoning": "LTO allows the compiler to inline and optimize across crate boundaries (e.g., bitboard attack functions called by engine search), which is critical for tight inner loops. Fewer codegen-units allow more holistic optimization at the cost of compile time; for chess engines, this tradeoff is typically worthwhile.",
    "measurement_approach": "Build with and without LTO/codegen-units tweaks and compare bench_search and occupancy_to_index benchmarks. Observe function size/inlining via cargo asm; expect fewer function calls in inner loops and better constant propagation."
  },
  {
    "id": "PERF-016",
    "title": "Apply PGO to focus optimization on search hotspots",
    "priority": "medium",
    "category": "compilation",
    "description": "",
    "status": "in-progress",
    "created_at": "2026-02-14T17:59:42.870608",
    "completed_at": null,
    "estimated_complexity": "large",
    "files_affected": [
      "Cargo.toml",
      "engine/benches/bench.rs"
    ],
    "dependencies": [],
    "consecutive_failures": 0,
    "current_bottleneck": "The build pipeline does not use Profile-Guided Optimization, so LLVM optimizes based on generic heuristics instead of real search workloads; branch prediction hints and inline decisions around alphabeta and movegen may be suboptimal.",
    "proposed_optimization": "Set up Rust PGO: build an instrumented binary, run bench_search and a suite of representative search workloads to collect profiles, then rebuild with profile-use. Target engine and bitboard crates in particular.",
    "expected_speedup": "10-30%",
    "requires_unsafe": "no",
    "requires_benchmarking": "yes",
    "reasoning": "Chess engines have irregular branch behavior (cutoffs, move ordering, quiescence) that generic static prediction may mis-handle. PGO allows LLVM to prioritize hot paths in search_node_with_arena, quiescence_with_arena, generate_legal_moves and eval, improving instruction layout, inlining, and devirtualization. Combined with LTO, this often yields sizable gains.",
    "measurement_approach": "Compile with and without PGO and run criterion benches (bench_search, perft) and real-match simulations. Compare nodes/sec, total time, and branch-miss counters from perf stat."
  }
]