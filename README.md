Cody: The AI-Authored Chess Engine
Cody is an experimental chess engine where every single line of code is generated by Artificial Intelligence. The goal of this project is to push the boundaries of LLM-assisted programming and answer one question: How strong of a chess engine can an AI build from scratch?

üöÄ The Philosophy
Unlike traditional engines (like Stockfish) or even NNUE-based engines tuned by humans, Cody is a "Zero-Human-Code" project.

Architecture & Logic: All search algorithms, evaluation functions, and bitboard manipulations are AI-generated.

Debugging: Human intervention is limited to providing error logs back to the AI for self-correction.

Goal: To achieve a competitive ELO rating while maintaining a codebase entirely birthed from prompt engineering.

üõ† Features (Current Status)
Bitboard Representation: High-performance board state management.

UCI Protocol Support: Compatible with standard chess GUIs like Arena, Cute Chess, or Scid vs. PC.

Search Algorithms: * Negamax with Alpha-Beta Pruning.

Iterative Deepening.

Quiescence Search to avoid the "horizon effect."

Evaluation: Hand-crafted AI heuristics including Piece-Square Tables (PST) and material imbalance weights.
---
## üß† How Cody Learns
Cody uses a recursive feedback loop. When the agent attempts a code improvement:
1. **Hypothesize:** The LLM suggests a chess logic improvement (e.g., "Implement Null Move Pruning").
2. **Execute:** `agent.py` applies the patch.
3. **Verify:** The local environment runs `cargo test`. 
4. **Refine:** If tests fail, Cody analyzes the compiler error and self-corrects.


How it works

Set environment variables:
SET OPENAI_KEY = "OPENAI_API_KEY"
SET GITHUB_TOKEN = "GITHUB_TOKEN"

## üè† Running Locally (Zero Cost & Private)
If you want to run Cody's improvement agent without an OpenAI account, you can use **Ollama**.

1.  **Install Ollama:** Download from [ollama.com](https://ollama.com/).
2.  **Pull a Model:**
    ```bash
    ollama pull deepseek-coder-v2:16b-lite-instruct-q4_K_M
    ```
3.  **Local Configuration:** In `config.json`, set `"use_local": true`. 
4.  **No API Key Needed:** When `use_local` is true, the agent uses a placeholder key to bypass the OpenAI login requirement.

Bash
ollama pull deepseek-coder-v2:16b-lite-instruct-q4_K_M
Start the Server: Ensure Ollama is running (it usually starts automatically in the system tray).

Configure Agent: In your config.json, set the following:

"model": "deepseek-coder-v2:16b-lite-instruct-q4_K_M"

"use_local": true (ensure your script is updated to handle this flag).

üíª Updated agent.py Logic
To make this work, we need to point the OpenAI client to your local machine instead of OpenAI's servers.

Step 1: Update your config.json
Add a base_url or a local flag so the script knows where to look.

JSON
{
    "branch_prefix": "ai-feature-",
    "model": "deepseek-coder-v2:16b-lite-instruct-q4_K_M",
    "api_base": "http://localhost:11434/v1"
}
Step 2: Modify agent.py
Find your call_ai function and update it to use the api_base from your config.

Python
# ... (existing imports)

# -----------------------------
# Updated Helper: call AI (Local or Cloud)
# -----------------------------
from openai import OpenAI

def call_ai(prompt):
    # Use the local URL if provided, otherwise default to OpenAI
    # api_key is required by the library but ignored by Ollama
    client = OpenAI(
        api_key=OPENAI_KEY if OPENAI_KEY else "ollama",
        base_url=CONFIG.get("api_base", "https://api.openai.com/v1")
    )

    print(f"Sending request to {CONFIG.get('model')} at {client.base_url}...")

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        temperature=0.4
    )
    return response.choices[0].message.content

## Human TODO items
1. Need PGO optimization
2. Need parameter tuning
3. Need SPRT testing
4. Need strong AI evaluation (premium models)

## üõ† License
MIT License ‚Äî see `LICENSE` for details.
